<!--
{
  "id": "transformers-2017",
  "title": "Transformers (2017)",
  "slug": "transformers-2017",
  "type": "concept",
  "era": "2010-talet",
  "domain": ["NLP","CV","Multimodal"],
  "tags": ["uppmärksamhet","arkitektur","sekvensmodellering","djupinlärning"],
  "hero": "/images/encyclopedia/transformers-2017/hero.webp",
  "level": "Intro",
  "related": ["gpt-family","self-attention"]
}
--><html><head></head><body><p class="lead" data-segment-id="transformers-2017-body-p-0">En självuppmärksam arkitektur som ersatte rekurrens med parallell uppmärksamhet, omformade NLP och möjliggjorde skalbara multimodala modeller.</p>
<div class="article-body">
  <p data-segment-id="transformers-2017-div-p-1">Transformers använder uppmärksamhet för att väga relationer mellan token, vilket möjliggör effektiv parallell träning och starka långdistansberoenden. Encoder-decoder- och enbart decoder-varianter ligger till grund för många ledande språk- och språk-bild-system.</p>
  <p data-segment-id="transformers-2017-div-p-2">Designen skalar väl med data och beräkningsresurser, vilket har drivit snabba framsteg i kapacitet och lett till bred användning inom forskning och industri.</p>
</div>


</body></html>