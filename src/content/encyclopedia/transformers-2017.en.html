<!--
{
  "id": "transformers-2017",
  "title": "Transformers (2017)",
  "slug": "transformers-2017",
  "type": "concept",
  "era": "2010s",
  "domain": ["NLP","CV","Multimodal"],
  "tags": ["attention","architecture","sequence-modeling","deep-learning"],
  "hero": "/images/encyclopedia/transformers-2017/hero.webp",
  "level": "Intro",
  "related": ["gpt-family","self-attention"]
}
--><html><head></head><body><p class="lead" data-segment-id="transformers-2017-body-p-0">A self-attention architecture that replaced recurrence with parallel attention, reshaping NLP and enabling scalable multimodal models.</p>
<div class="article-body">
  <p data-segment-id="transformers-2017-div-p-1">Transformers use attention to weight relationships among tokens, allowing efficient parallel training and strong long-range dependencies. Encoder-decoder and decoder-only variants underpin many state-of-the-art language and vision-language systems.</p>
  <p data-segment-id="transformers-2017-div-p-2">The design scales effectively with data and compute, which has driven rapid advances in capabilities and sparked broad adoption across research and industry.</p>
</div>


</body></html>