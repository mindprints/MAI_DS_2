<!--
{
  "id": "backpropagation-1986",
  "title": "Backpropagation (1986)",
  "slug": "backpropagation-1986",
  "type": "concept",
  "era": "1980-talet",
  "domain": ["ML"],
  "tags": ["träning","gradienter","flerskikt","djupinlärning"],
  "hero": "/images/encyclopedia/backpropagation-1986/hero.webp",
  "level": "Intro",
  "related": ["perceptron","transformers-2017"]
}
--><html><head></head><body><p class="lead" data-segment-id="backpropagation-1986-body-p-0">En effektiv metod för att beräkna gradienter genom flerskiktsnät, vilket möjliggör praktisk träning bortom enskiktsmodeller.</p>
<div class="article-body">
  <p data-segment-id="backpropagation-1986-div-p-1">Backpropagation tillämpar kedjeregeln lager för lager för att sprida fel från utgångar tillbaka till tidigare lager. Det gör det möjligt att optimera många parametrar med gradientnedstigning och återupplivade intresset för neurala nät efter tidigare begränsningar.</p>
  <p data-segment-id="backpropagation-1986-div-p-2">Tekniken lade grunden för modern djupinlärning, där större dataset, beräkningskraft och förbättrade arkitekturer förlänger samma kärnprincip för träning.</p>
</div>


</body></html>